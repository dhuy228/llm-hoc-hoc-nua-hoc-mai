# Attemp to understand the back-end flow when calling Agent

### `run` ‚Äî Workflow Execution

Runs a workflow starting at the given agent.  
The agent executes in a loop until a **final output** is generated.

---

#### Loop Execution Flow
1. **Invoke agent** with the given input.  
2. **Check for final output**:  
   - If the agent produces something of type `agent.output_type`, the loop terminates.  
3. **Check for handoff**:  
   - If there‚Äôs a handoff, the loop continues with the new agent.  
4. **Else, run tool calls** (if any), then re-run the loop.  

---

#### Exceptions
The agent may raise an exception in two cases:  
1. **`MaxTurnsExceeded`** ‚Üí Raised if `max_turns` is exceeded.  
   - A turn = one AI invocation (including any tool calls).  
2. **`GuardrailTripwireTriggered`** ‚Üí Raised if a guardrail tripwire is triggered.  
   - Note: Only the **first agent‚Äôs input guardrails** are run.  

---

#### Arguments
- **`starting_agent`** ‚Üí The starting agent to run.  
- **`input`** ‚Üí The initial input to the agent.  
  - Can be a single string (user message) or a list of input items.  
- **`context`** ‚Üí The context to run the agent with.  
- **`max_turns`** ‚Üí Maximum number of turns to run the agent.  
- **`hooks`** ‚Üí Object that receives callbacks on lifecycle events.  
- **`run_config`** ‚Üí Global settings for the entire agent run.  
- **`previous_response_id`** ‚Üí ID of the previous response (if using OpenAI models via the Responses API).  
  - Allows skipping re-passing input from the previous turn.  

---

#### Returns
A **run result** containing:  
- All inputs  
- Guardrail results  
- Output of the last agent  

> Since agents may perform handoffs, the **exact output type is not guaranteed**.  

---

Do you want me to keep this in **docstring style** (triple quotes, ready for Python code), or more like a **Markdown reference doc** (like above)?  

---

## 5. User
how does it know when a final output is generated?  

---

## 6. Assistant
Good question üëå  

It knows a **final output is generated** when the agent produces something that matches the agent‚Äôs declared **`output_type`**.  

Here‚Äôs the flow broken down:

- Each agent has an **`output_type`** (e.g. a structured schema, JSON, or some other expected type).  
- During the loop, after the agent is invoked, the system checks:  
  - **Does the agent‚Äôs response conform to `agent.output_type`?**  
    - ‚úÖ If yes ‚Üí This is considered the **final output**, and the loop **terminates**.  
    - ‚ùå If no ‚Üí The loop continues (either with a handoff to another agent, or by executing tool calls).  

So, the **contract** between the agent and the workflow runner is:  
> *‚ÄúWhen I emit something of my `output_type`, that‚Äôs my final answer.‚Äù*  

Would you like me to show you a **concrete Python example** where an agent produces its final output (vs a tool call or handoff), so you can see how that check happens in practice?  

---

## 7. User
but by the default, it returns str so how does it know when to end the loop?  

---

## 8. Assistant
Exactly ‚Äî by default, most agents are typed as producing a `str` for their `output_type`.  

So how does the runner know *this* string is the ‚Äúfinal output‚Äù instead of just an intermediate step like a tool call?  

Here‚Äôs what happens under the hood:  

1. **The runner inspects the agent‚Äôs response.**  
   - If the response is a plain string (and the agent‚Äôs `output_type` is `str`), that **is considered a final output**.  
   - The loop stops immediately.  

2. **If the response is not a string**, the runner checks:  
   - Is it a *handoff* (telling the system to pass control to another agent)?  
   - Is it a *tool call* request?  
   - If yes, the loop continues with those steps.  

So in the ‚Äúdefault `str` output_type‚Äù case:  
- **Any normal string output means ‚ÄúI‚Äôm done‚Äù**.  
- The only way the loop continues is if the agent explicitly returns a handoff or tool call instead of a string.  

<img width="720" height="519" alt="image" src="https://github.com/user-attachments/assets/a6812748-4543-4d87-9ed5-e4785853ec64" />

# Building Reliable Agent

**I think the best thing that Agents can do now reliably is to return structured output then you can use it to run a deterministic workflow.
Long-running agent is a different beast. It does that continously in a loop until the goal is met. Which means if your context is "Garbage", prepare to have an exponential growth of "Garbage Out".

-----
The most reliable agents are built with the same fundamentals: clear logic, good structure, and tight feedback loops.
https://vercel.com/blog/the-no-nonsense-approach-to-ai-agent-development#step-2:-automate-the-loop

Before committing to any implementation, test the task manually using real inputs and prompts. If the model can almost get there with help, it‚Äôs probably worth building. From there, structure the logic using ordinary code and only rely on the model when necessary. Once the agent is working, focus on making it resilient. Both on individual inputs and across real-world complexity.

To recap, agents are useful when:

The task is difficult to automate with traditional code

Prompting the model manually shows signs of success

You scope the agent narrowly and build on solid software practices

You optimize both with hands-on intuition and structured evaluation
